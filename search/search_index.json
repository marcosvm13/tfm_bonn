{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Perfecto. Aqu\u00ed tienes una versi\u00f3n m\u00e1s sobria, profesional y llamativa del resumen inicial del proyecto BONN, sin iconos ni referencias expl\u00edcitas al dataset o a la inspiraci\u00f3n biol\u00f3gica, pero con un tono atractivo y t\u00e9cnico: BONN: Redes Neuronales Eficientes para Aprendizaje R\u00e1pido BONN (Bio-inspired Olfactory Neural Network) es una arquitectura dise\u00f1ada para entornos donde la eficiencia, la rapidez de entrenamiento y la robustez son prioritarias. Frente a los modelos tradicionales, BONN ofrece una alternativa ligera y altamente competitiva en escenarios de clasificaci\u00f3n con pocos datos y entrenamiento superficial. Este repositorio implementa y analiza una familia de modelos que combinan conectividad estructurada fija con mecanismos de activaci\u00f3n controlada, obteniendo representaciones internas especializadas sin necesidad de m\u00faltiples capas entrenables. Caracter\u00edsticas destacadas Arquitectura con una sola capa entrenable y conexiones fijas. Entrenamiento eficiente: convergencia en una \u00fanica \u00e9poca. Codificaci\u00f3n dispersa y representaciones especializadas emergentes. Alta tolerancia al pruning sin necesidad de reentrenamiento. Extensiones con activaci\u00f3n estoc\u00e1stica y agregaci\u00f3n estructurada. Modelos incluidos BONN : Modelo base con proyecciones aleatorias fijas y salida entrenable. GateBONN : Variante con compuertas estoc\u00e1sticas diferenciables y regularizaci\u00f3n bayesiana. EnsembleBONN : Combinaci\u00f3n de m\u00faltiples instancias independientes para mejorar estabilidad y precisi\u00f3n. Comparativas con modelos de referencia: MLP , ELM , RFM . Estructura del proyecto src/ \u251c\u2500\u2500 bonn/ # Arquitectura principal BONN \u251c\u2500\u2500 gatebonn/ # GateBONN: versi\u00f3n con activaci\u00f3n estoc\u00e1stica \u251c\u2500\u2500 ensemble/ # L\u00f3gica de ensamble y agregaci\u00f3n \u251c\u2500\u2500 training/ # Entrenamiento, validaci\u00f3n y evaluaci\u00f3n \u251c\u2500\u2500 analysis/ # Visualizaci\u00f3n de activaciones, escasez y especializaci\u00f3n \u251c\u2500\u2500 experiments/ # Scripts para ejecuci\u00f3n de experimentos controlados \u2514\u2500\u2500 app.py # Punto de entrada para lanzar entrenamientos Documentaci\u00f3n Installation : C\u00f3mo instalar y configurar el entorno Usage : Ejecuci\u00f3n de los experimentos Technical Details : Dise\u00f1o interno de los modelos y su justificaci\u00f3n biol\u00f3gica","title":"Inicio"},{"location":"#bonn-redes-neuronales-eficientes-para-aprendizaje-rapido","text":"BONN (Bio-inspired Olfactory Neural Network) es una arquitectura dise\u00f1ada para entornos donde la eficiencia, la rapidez de entrenamiento y la robustez son prioritarias. Frente a los modelos tradicionales, BONN ofrece una alternativa ligera y altamente competitiva en escenarios de clasificaci\u00f3n con pocos datos y entrenamiento superficial. Este repositorio implementa y analiza una familia de modelos que combinan conectividad estructurada fija con mecanismos de activaci\u00f3n controlada, obteniendo representaciones internas especializadas sin necesidad de m\u00faltiples capas entrenables.","title":"BONN: Redes Neuronales Eficientes para Aprendizaje R\u00e1pido"},{"location":"#caracteristicas-destacadas","text":"Arquitectura con una sola capa entrenable y conexiones fijas. Entrenamiento eficiente: convergencia en una \u00fanica \u00e9poca. Codificaci\u00f3n dispersa y representaciones especializadas emergentes. Alta tolerancia al pruning sin necesidad de reentrenamiento. Extensiones con activaci\u00f3n estoc\u00e1stica y agregaci\u00f3n estructurada.","title":"Caracter\u00edsticas destacadas"},{"location":"#modelos-incluidos","text":"BONN : Modelo base con proyecciones aleatorias fijas y salida entrenable. GateBONN : Variante con compuertas estoc\u00e1sticas diferenciables y regularizaci\u00f3n bayesiana. EnsembleBONN : Combinaci\u00f3n de m\u00faltiples instancias independientes para mejorar estabilidad y precisi\u00f3n. Comparativas con modelos de referencia: MLP , ELM , RFM .","title":"Modelos incluidos"},{"location":"#estructura-del-proyecto","text":"src/ \u251c\u2500\u2500 bonn/ # Arquitectura principal BONN \u251c\u2500\u2500 gatebonn/ # GateBONN: versi\u00f3n con activaci\u00f3n estoc\u00e1stica \u251c\u2500\u2500 ensemble/ # L\u00f3gica de ensamble y agregaci\u00f3n \u251c\u2500\u2500 training/ # Entrenamiento, validaci\u00f3n y evaluaci\u00f3n \u251c\u2500\u2500 analysis/ # Visualizaci\u00f3n de activaciones, escasez y especializaci\u00f3n \u251c\u2500\u2500 experiments/ # Scripts para ejecuci\u00f3n de experimentos controlados \u2514\u2500\u2500 app.py # Punto de entrada para lanzar entrenamientos","title":"Estructura del proyecto"},{"location":"#documentacion","text":"Installation : C\u00f3mo instalar y configurar el entorno Usage : Ejecuci\u00f3n de los experimentos Technical Details : Dise\u00f1o interno de los modelos y su justificaci\u00f3n biol\u00f3gica","title":"Documentaci\u00f3n"},{"location":"installation/","text":"Installation Stable release To install tfm_bonn, run this command in your terminal: pip install tfm_bonn This is the preferred method to install tfm_bonn, as it will always install the most recent stable release. If you don't have pip installed, this Python installation guide can guide you through the process. From sources The sources for tfm_bonn can be downloaded from the Github repo . You can either clone the public repository: git clone git://github.com/marcosvm13/tfm_bonn Or download the tarball : curl -OJL https://github.com/marcosvm13/tfm_bonn/tarball/master Once you have a copy of the source, you can install it with: python setup.py install Instalaci\u00f3n del proyecto BONN Este proyecto puede ejecutarse en entornos con GPU o CPU, aunque se recomienda el uso de GPU con soporte CUDA para experimentar con CuPy y PyTorch eficientemente. Requisitos Python 3.9 o superior CUDA (si se usa GPU + CuPy) Instalaci\u00f3n con entorno virtual python -m venv venv source venv/bin/activate # En Windows: venv\\Scripts\\activate pip install --upgrade pip pip install -r requirements.txt CuPy seg\u00fan tu versi\u00f3n de CUDA Instala cupy correspondiente a tu versi\u00f3n CUDA (verifica con nvcc --version ): pip install cupy-cuda12x # Para CUDA 12.x # o pip install cupy-cuda11x # Para CUDA 11.x Si no dispones de GPU, puedes eliminar cupy del proyecto y reemplazarlo por numpy en el c\u00f3digo. Descarga de datasets Los experimentos utilizan MNIST. PyTorch lo descarga autom\u00e1ticamente en la primera ejecuci\u00f3n. ./data/ # Se crea al correr los scripts por primera vez Prueba r\u00e1pida python src/app.py Esto ejecuta un pipeline de ejemplo con la arquitectura BONN.","title":"Instalaci\u00f3n"},{"location":"installation/#installation","text":"","title":"Installation"},{"location":"installation/#stable-release","text":"To install tfm_bonn, run this command in your terminal: pip install tfm_bonn This is the preferred method to install tfm_bonn, as it will always install the most recent stable release. If you don't have pip installed, this Python installation guide can guide you through the process.","title":"Stable release"},{"location":"installation/#from-sources","text":"The sources for tfm_bonn can be downloaded from the Github repo . You can either clone the public repository: git clone git://github.com/marcosvm13/tfm_bonn Or download the tarball : curl -OJL https://github.com/marcosvm13/tfm_bonn/tarball/master Once you have a copy of the source, you can install it with: python setup.py install","title":"From sources"},{"location":"installation/#instalacion-del-proyecto-bonn","text":"Este proyecto puede ejecutarse en entornos con GPU o CPU, aunque se recomienda el uso de GPU con soporte CUDA para experimentar con CuPy y PyTorch eficientemente.","title":"Instalaci\u00f3n del proyecto BONN"},{"location":"installation/#requisitos","text":"Python 3.9 o superior CUDA (si se usa GPU + CuPy)","title":"Requisitos"},{"location":"installation/#instalacion-con-entorno-virtual","text":"python -m venv venv source venv/bin/activate # En Windows: venv\\Scripts\\activate pip install --upgrade pip pip install -r requirements.txt","title":"Instalaci\u00f3n con entorno virtual"},{"location":"installation/#cupy-segun-tu-version-de-cuda","text":"Instala cupy correspondiente a tu versi\u00f3n CUDA (verifica con nvcc --version ): pip install cupy-cuda12x # Para CUDA 12.x # o pip install cupy-cuda11x # Para CUDA 11.x Si no dispones de GPU, puedes eliminar cupy del proyecto y reemplazarlo por numpy en el c\u00f3digo.","title":"CuPy seg\u00fan tu versi\u00f3n de CUDA"},{"location":"installation/#descarga-de-datasets","text":"Los experimentos utilizan MNIST. PyTorch lo descarga autom\u00e1ticamente en la primera ejecuci\u00f3n. ./data/ # Se crea al correr los scripts por primera vez","title":"Descarga de datasets"},{"location":"installation/#prueba-rapida","text":"python src/app.py Esto ejecuta un pipeline de ejemplo con la arquitectura BONN.","title":"Prueba r\u00e1pida"},{"location":"technical/","text":"Documentaci\u00f3n T\u00e9cnica del Proyecto BONN Este repositorio implementa modelos bioinspirados en el sistema olfativo de los insectos, enfocados en tareas de clasificaci\u00f3n con pocos datos. Las arquitecturas propuestas reproducen mecanismos funcionales clave del procesamiento sensorial en Drosophila melanogaster , como la codificaci\u00f3n dispersa, la especializaci\u00f3n funcional y la robustez estructural. 1. Arquitectura BONN BONN (Bio-inspired Olfactory Neural Network) simula la v\u00eda sensorial AL \u2192 KC \u2192 MBON. Su estructura consta de: Proyecciones fijas desde AL a KC : Representadas mediante matrices de conexi\u00f3n binarias, dispersas y parcialmente estructuradas. Esta matriz se inicializa una \u00fanica vez al comienzo y no se entrena. Codificaci\u00f3n dispersa : Controlada mediante dos hiperpar\u00e1metros: s : tasa de activaci\u00f3n objetivo para cada entrada. pc : probabilidad de conexi\u00f3n de cada neurona de KC con entradas del AL. Salida entrenable (MBON) : Una \u00fanica capa lineal entrenada con descenso por gradiente. Funci\u00f3n de activaci\u00f3n : Sigmoide o variantes suaves, usada para mantener la interpretabilidad de las tasas de activaci\u00f3n. La implementaci\u00f3n principal est\u00e1 escrita en CuPy para acelerar la ejecuci\u00f3n mediante operaciones vectorizadas en GPU. El uso de m\u00e1scaras binarias permite simular la conectividad estructural AL\u2192KC de forma eficiente. 2. GateBONN (versi\u00f3n bayesiana con compuertas) GateBONN extiende BONN incorporando mecanismos bayesianos de control de activaci\u00f3n: Compuertas estoc\u00e1sticas diferenciables : Se utiliza la relajaci\u00f3n de Gumbel-Sigmoid para permitir el paso de gradientes en compuertas binarias. Distribuciones beta : Cada compuerta tiene una distribuci\u00f3n Beta con par\u00e1metros aprendidos. Esto permite modelar expl\u00edcitamente la probabilidad de activaci\u00f3n de cada neurona. Regularizaci\u00f3n por divergencia KL : Se a\u00f1ade un t\u00e9rmino en la funci\u00f3n de p\u00e9rdida que penaliza desviaciones respecto a un prior Beta, promoviendo escasez y control de entrop\u00eda. C\u00e1lculo de informaci\u00f3n mutua : Se estima la dependencia entre las activaciones internas y las clases objetivo usando un t\u00e9rmino tipo InfoNCE o una medida basada en codificaci\u00f3n cruzada. Este modelo est\u00e1 implementado completamente en PyTorch, aprovechando su compatibilidad con autograd y su ecosistema de entrenamiento distribuido. 3. EnsembleBONN Agregaci\u00f3n de modelos BONN independientes : Se generan m\u00faltiples instancias de BONN con distintas inicializaciones estructurales (m\u00e1scaras de conexi\u00f3n). Votaci\u00f3n suave (soft voting) : Las salidas de cada modelo se combinan promediando las probabilidades clase a clase. Evaluaci\u00f3n progresiva : Se mide c\u00f3mo mejora la precisi\u00f3n del conjunto a medida que se agregan modelos, desde 1 hasta 100. Motivaci\u00f3n biol\u00f3gica : Refleja la diversidad funcional de subpoblaciones KCs en el MB (\u03b1, \u03b2, \u03b3). 4. Experimentos y Evaluaci\u00f3n Se han llevado a cabo varios experimentos para validar el rendimiento y las propiedades internas de BONN y sus variantes: Comparaci\u00f3n con modelos cl\u00e1sicos : Se comparan BONN, GateBONN y EnsembleBONN con MLP, ELM y RFM bajo entrenamiento superficial (una \u00e9poca). Fast learning : Evaluaci\u00f3n de precisi\u00f3n en funci\u00f3n del n\u00famero de muestras de entrenamiento, desde 1 hasta 10k. Pruning sin reentrenamiento : Evaluaci\u00f3n de robustez al eliminar un porcentaje creciente de neuronas ocultas sin volver a entrenar. Especializaci\u00f3n funcional : An\u00e1lisis de la proporci\u00f3n de KCs especialistas y generalistas a trav\u00e9s de m\u00e9tricas basadas en la sensibilidad por clase. Multimodalidad de activaciones : Se estudia la distribuci\u00f3n del n\u00famero efectivo de clases que activan cada KC, buscando indicios de subpoblaciones funcionales diferenciadas. 5. Entradas y Salidas Datos : MNIST (grises, 28\u00d728 p\u00edxeles, 10 clases). Preprocesamiento : Flatten + normalizaci\u00f3n a [0, 1]. Entrada del modelo : vectores de 784 dimensiones. Salida del modelo : logits (10 dimensiones), convertidos a probabilidades v\u00eda softmax. M\u00e9tricas : accuracy, sparsity media, especializaci\u00f3n funcional, robustez al pruning. 6. Implementaci\u00f3n t\u00e9cnica Componente Librer\u00eda Detalles t\u00e9cnicos BONN CuPy M\u00e1scaras binarias, operaciones vectorizadas en GPU GateBONN PyTorch Uso de torch.distributions.Beta , RelaxedBernoulli , nn.Module Pruning NumPy / CuPy Eliminaci\u00f3n de neuronas seg\u00fan \u00edndice Visualizaci\u00f3n Matplotlib Histogramas de especializaci\u00f3n, evoluci\u00f3n de precisi\u00f3n Ficheros clave bonn.py : clase BONN (CuPy) gatebonn.py : clase GateBONN (PyTorch) ensemble.py : funciones para crear y evaluar ensambles experiments/ : scripts de evaluaci\u00f3n r\u00e1pida, pruning y an\u00e1lisis interno utils/ : funciones de activaci\u00f3n, inicializaci\u00f3n y an\u00e1lisis de activaciones 7. Requisitos cupy>=11.0.0 torch>=2.0.0 matplotlib scikit-learn scipy Para un an\u00e1lisis m\u00e1s detallado del marco conceptual, referirse al Cap\u00edtulo de Desarrollo del TFM. All\u00ed se explican las motivaciones biol\u00f3gicas, las propiedades funcionales y los resultados experimentales con referencias cruzadas entre secciones.","title":"Detalles T\u00e9cnicos"},{"location":"technical/#documentacion-tecnica-del-proyecto-bonn","text":"Este repositorio implementa modelos bioinspirados en el sistema olfativo de los insectos, enfocados en tareas de clasificaci\u00f3n con pocos datos. Las arquitecturas propuestas reproducen mecanismos funcionales clave del procesamiento sensorial en Drosophila melanogaster , como la codificaci\u00f3n dispersa, la especializaci\u00f3n funcional y la robustez estructural.","title":"Documentaci\u00f3n T\u00e9cnica del Proyecto BONN"},{"location":"technical/#1-arquitectura-bonn","text":"BONN (Bio-inspired Olfactory Neural Network) simula la v\u00eda sensorial AL \u2192 KC \u2192 MBON. Su estructura consta de: Proyecciones fijas desde AL a KC : Representadas mediante matrices de conexi\u00f3n binarias, dispersas y parcialmente estructuradas. Esta matriz se inicializa una \u00fanica vez al comienzo y no se entrena. Codificaci\u00f3n dispersa : Controlada mediante dos hiperpar\u00e1metros: s : tasa de activaci\u00f3n objetivo para cada entrada. pc : probabilidad de conexi\u00f3n de cada neurona de KC con entradas del AL. Salida entrenable (MBON) : Una \u00fanica capa lineal entrenada con descenso por gradiente. Funci\u00f3n de activaci\u00f3n : Sigmoide o variantes suaves, usada para mantener la interpretabilidad de las tasas de activaci\u00f3n. La implementaci\u00f3n principal est\u00e1 escrita en CuPy para acelerar la ejecuci\u00f3n mediante operaciones vectorizadas en GPU. El uso de m\u00e1scaras binarias permite simular la conectividad estructural AL\u2192KC de forma eficiente.","title":"1. Arquitectura BONN"},{"location":"technical/#2-gatebonn-version-bayesiana-con-compuertas","text":"GateBONN extiende BONN incorporando mecanismos bayesianos de control de activaci\u00f3n: Compuertas estoc\u00e1sticas diferenciables : Se utiliza la relajaci\u00f3n de Gumbel-Sigmoid para permitir el paso de gradientes en compuertas binarias. Distribuciones beta : Cada compuerta tiene una distribuci\u00f3n Beta con par\u00e1metros aprendidos. Esto permite modelar expl\u00edcitamente la probabilidad de activaci\u00f3n de cada neurona. Regularizaci\u00f3n por divergencia KL : Se a\u00f1ade un t\u00e9rmino en la funci\u00f3n de p\u00e9rdida que penaliza desviaciones respecto a un prior Beta, promoviendo escasez y control de entrop\u00eda. C\u00e1lculo de informaci\u00f3n mutua : Se estima la dependencia entre las activaciones internas y las clases objetivo usando un t\u00e9rmino tipo InfoNCE o una medida basada en codificaci\u00f3n cruzada. Este modelo est\u00e1 implementado completamente en PyTorch, aprovechando su compatibilidad con autograd y su ecosistema de entrenamiento distribuido.","title":"2. GateBONN (versi\u00f3n bayesiana con compuertas)"},{"location":"technical/#3-ensemblebonn","text":"Agregaci\u00f3n de modelos BONN independientes : Se generan m\u00faltiples instancias de BONN con distintas inicializaciones estructurales (m\u00e1scaras de conexi\u00f3n). Votaci\u00f3n suave (soft voting) : Las salidas de cada modelo se combinan promediando las probabilidades clase a clase. Evaluaci\u00f3n progresiva : Se mide c\u00f3mo mejora la precisi\u00f3n del conjunto a medida que se agregan modelos, desde 1 hasta 100. Motivaci\u00f3n biol\u00f3gica : Refleja la diversidad funcional de subpoblaciones KCs en el MB (\u03b1, \u03b2, \u03b3).","title":"3. EnsembleBONN"},{"location":"technical/#4-experimentos-y-evaluacion","text":"Se han llevado a cabo varios experimentos para validar el rendimiento y las propiedades internas de BONN y sus variantes: Comparaci\u00f3n con modelos cl\u00e1sicos : Se comparan BONN, GateBONN y EnsembleBONN con MLP, ELM y RFM bajo entrenamiento superficial (una \u00e9poca). Fast learning : Evaluaci\u00f3n de precisi\u00f3n en funci\u00f3n del n\u00famero de muestras de entrenamiento, desde 1 hasta 10k. Pruning sin reentrenamiento : Evaluaci\u00f3n de robustez al eliminar un porcentaje creciente de neuronas ocultas sin volver a entrenar. Especializaci\u00f3n funcional : An\u00e1lisis de la proporci\u00f3n de KCs especialistas y generalistas a trav\u00e9s de m\u00e9tricas basadas en la sensibilidad por clase. Multimodalidad de activaciones : Se estudia la distribuci\u00f3n del n\u00famero efectivo de clases que activan cada KC, buscando indicios de subpoblaciones funcionales diferenciadas.","title":"4. Experimentos y Evaluaci\u00f3n"},{"location":"technical/#5-entradas-y-salidas","text":"Datos : MNIST (grises, 28\u00d728 p\u00edxeles, 10 clases). Preprocesamiento : Flatten + normalizaci\u00f3n a [0, 1]. Entrada del modelo : vectores de 784 dimensiones. Salida del modelo : logits (10 dimensiones), convertidos a probabilidades v\u00eda softmax. M\u00e9tricas : accuracy, sparsity media, especializaci\u00f3n funcional, robustez al pruning.","title":"5. Entradas y Salidas"},{"location":"technical/#6-implementacion-tecnica","text":"Componente Librer\u00eda Detalles t\u00e9cnicos BONN CuPy M\u00e1scaras binarias, operaciones vectorizadas en GPU GateBONN PyTorch Uso de torch.distributions.Beta , RelaxedBernoulli , nn.Module Pruning NumPy / CuPy Eliminaci\u00f3n de neuronas seg\u00fan \u00edndice Visualizaci\u00f3n Matplotlib Histogramas de especializaci\u00f3n, evoluci\u00f3n de precisi\u00f3n","title":"6. Implementaci\u00f3n t\u00e9cnica"},{"location":"technical/#ficheros-clave","text":"bonn.py : clase BONN (CuPy) gatebonn.py : clase GateBONN (PyTorch) ensemble.py : funciones para crear y evaluar ensambles experiments/ : scripts de evaluaci\u00f3n r\u00e1pida, pruning y an\u00e1lisis interno utils/ : funciones de activaci\u00f3n, inicializaci\u00f3n y an\u00e1lisis de activaciones","title":"Ficheros clave"},{"location":"technical/#7-requisitos","text":"cupy>=11.0.0 torch>=2.0.0 matplotlib scikit-learn scipy Para un an\u00e1lisis m\u00e1s detallado del marco conceptual, referirse al Cap\u00edtulo de Desarrollo del TFM. All\u00ed se explican las motivaciones biol\u00f3gicas, las propiedades funcionales y los resultados experimentales con referencias cruzadas entre secciones.","title":"7. Requisitos"},{"location":"usage/","text":"Uso del proyecto BONN A continuaci\u00f3n se detallan ejemplos de c\u00f3mo ejecutar diferentes experimentos y modelos desde el repositorio. 1. Ejecutar BONN con entrenamiento superficial python src/app.py Este script entrena el modelo BONN con una sola capa entrenable tras proyecciones fijas y mide su precisi\u00f3n. 2. Ejecutar GateBONN (PyTorch) python src/gatebonn/gatebonn_experiment.py Este script entrena GateBONN sobre MNIST para distintos tama\u00f1os de entrenamiento y devuelve un DataFrame con la precisi\u00f3n media y desviaci\u00f3n est\u00e1ndar. 3. Evaluar ensambles BONN from src.ensemble.ensemble_bonn import EnsembleBONN Puedes construir un ensamble de modelos BONN con: ensemble = EnsembleBONN(fit_fn=my_fit_function) accs = ensemble.evaluate_ensemble(X_test, y_test, ensemble_sizes=[1, 2, 5, 10]) 4. An\u00e1lisis de activaci\u00f3n y escasez from src.analysis.multimodality import analyze_kc_digit_multimodality from src.analysis.neuron_ranking import kill_progressive_neurons Estos m\u00f3dulos permiten medir multimodalidad y simular pruning sobre las KCs. 5. Visualizaci\u00f3n de resultados from src.analysis.plot_utils import plot_surfaces_adjusted from src.analysis.pruning_analysis import plot_pruning_progress_stylized Las funciones de visualizaci\u00f3n permiten comparar arquitecturas y evaluar degradaci\u00f3n de precisi\u00f3n ante eliminaci\u00f3n de neuronas.","title":"Uso"},{"location":"usage/#uso-del-proyecto-bonn","text":"A continuaci\u00f3n se detallan ejemplos de c\u00f3mo ejecutar diferentes experimentos y modelos desde el repositorio.","title":"Uso del proyecto BONN"},{"location":"usage/#1-ejecutar-bonn-con-entrenamiento-superficial","text":"python src/app.py Este script entrena el modelo BONN con una sola capa entrenable tras proyecciones fijas y mide su precisi\u00f3n.","title":"1. Ejecutar BONN con entrenamiento superficial"},{"location":"usage/#2-ejecutar-gatebonn-pytorch","text":"python src/gatebonn/gatebonn_experiment.py Este script entrena GateBONN sobre MNIST para distintos tama\u00f1os de entrenamiento y devuelve un DataFrame con la precisi\u00f3n media y desviaci\u00f3n est\u00e1ndar.","title":"2. Ejecutar GateBONN (PyTorch)"},{"location":"usage/#3-evaluar-ensambles-bonn","text":"from src.ensemble.ensemble_bonn import EnsembleBONN Puedes construir un ensamble de modelos BONN con: ensemble = EnsembleBONN(fit_fn=my_fit_function) accs = ensemble.evaluate_ensemble(X_test, y_test, ensemble_sizes=[1, 2, 5, 10])","title":"3. Evaluar ensambles BONN"},{"location":"usage/#4-analisis-de-activacion-y-escasez","text":"from src.analysis.multimodality import analyze_kc_digit_multimodality from src.analysis.neuron_ranking import kill_progressive_neurons Estos m\u00f3dulos permiten medir multimodalidad y simular pruning sobre las KCs.","title":"4. An\u00e1lisis de activaci\u00f3n y escasez"},{"location":"usage/#5-visualizacion-de-resultados","text":"from src.analysis.plot_utils import plot_surfaces_adjusted from src.analysis.pruning_analysis import plot_pruning_progress_stylized Las funciones de visualizaci\u00f3n permiten comparar arquitecturas y evaluar degradaci\u00f3n de precisi\u00f3n ante eliminaci\u00f3n de neuronas.","title":"5. Visualizaci\u00f3n de resultados"}]}